<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-10T09:22:01+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nanbei629 blog</title><subtitle>Record</subtitle><author><name>true</name></author><entry><title type="html">树(一) —- 知识体系</title><link href="http://localhost:4000/2020/03/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A0%91-%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB.html" rel="alternate" type="text/html" title="树(一) ---- 知识体系" /><published>2020-03-31T00:00:00+08:00</published><updated>2020-03-31T00:00:00+08:00</updated><id>http://localhost:4000/2020/03/31/%5B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%5D%20%E6%A0%91%E2%80%94%E2%80%94%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB</id><content type="html" xml:base="http://localhost:4000/2020/03/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A0%91-%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB.html">&lt;p&gt;&lt;img src=&quot;/assets/img/tree.png&quot; /&gt;&lt;/p&gt;</content><author><name>nanbei629</name></author><category term="机器学习" /><category term="树" /><summary type="html"></summary></entry><entry><title type="html">树(二) —- 决策树之特征选择</title><link href="http://localhost:4000/2020/03/31/%E6%A0%91-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91.html" rel="alternate" type="text/html" title="树(二) ---- 决策树之特征选择" /><published>2020-03-31T00:00:00+08:00</published><updated>2020-03-31T00:00:00+08:00</updated><id>http://localhost:4000/2020/03/31/%E6%A0%91(%E4%BA%8C)%E5%86%B3%E7%AD%96%E6%A0%91</id><content type="html" xml:base="http://localhost:4000/2020/03/31/%E6%A0%91-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91.html">&lt;p&gt;决策树是一种有监督模型，既能分类也能用于回归，既可以是二叉树，也可以是非二叉树。决策树的叶子结点表示结果，非叶子结点表示特征。建立决策树的过程主要是通过算法递归地为每个非叶子结点选择最优特征；建立后经过剪枝操作，将因为数据noise 和outlier生成的分支移除，以此提升树的精确度和泛化能力。决策树因为可读性强，计算快而被广泛应用。&lt;/p&gt;

&lt;p&gt;这一篇主要介绍不同的特征选择方法，还有这些方法如何处理连续值与缺失值。&lt;/p&gt;

&lt;h3 id=&quot;特征选择&quot;&gt;特征选择&lt;/h3&gt;

&lt;p&gt;方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;信息增益（Gain）&lt;/li&gt;
  &lt;li&gt;信息增益比 （Gain ratio）&lt;/li&gt;
  &lt;li&gt;基尼系数 （Gini index）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对信息增益和信息增益比这两种方法，我们的目标是，让经过特征结点分类后的数据是比较确定的，而不是随机的。在这个过程中引入信息熵作为选择结点特征的基础。信息熵是用来衡量随机变量$X$的不确定性，公式为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}H(x) = -\sum_{x}p(x)logp(x)\end{gather}&lt;/script&gt;

&lt;p&gt;由公式可以看出信息熵的特点，&lt;strong&gt;发生概率越高的事件即确定性事件，它的信息熵越小&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;信息增益&quot;&gt;信息增益&lt;/h4&gt;

&lt;p&gt;信息增益用来比较数据集经过某个特征分类后的信息熵变化，如果经过分类后，信息熵变的更小即分类后的数据更加确定，说明选择的特征分化能力较强，计算信息增益的过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;计算数据集D的信息熵&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}Info(D) =  -\sum_{i}^mp_ilog_2p_i,\quad p_i = \frac{|C_{i,D}|}{D}\end{gather}&lt;/script&gt;

    &lt;p&gt;其中$p_i$ 是数据集中第$i$类的概率&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算数据集D经特征A分类后的信息熵&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}Info_A(D) = \sum_{j=1}^v \frac{|D_j|}{|D|}*Info(D_j)\end{gather}&lt;/script&gt;

    &lt;p&gt;其中 $v$ 是特征A中不同的值 的个数${a_1,a_2,…,a_v}$ , 这些值将数据集$D$划分为$v$个数据子集&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算特征A的信息增益值&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}Gain(A) = Info(D) - Info_A(D)\end{gather}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$Info_A(D)$越小，分类后的结果越确定，$Gain(A)$就越大&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;信息增益很直观的衡量了特征的分化能力，但是这种方法也存在弊端，&lt;strong&gt;它会偏向于选取值较多的特征&lt;/strong&gt;。比如对用户数据，我们将用户$ID$ 看作一个特征(A)，来计算&lt;script type=&quot;math/tex&quot;&gt;Gain(ID)&lt;/script&gt;，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;n个用户ID将数据集划分为n个数据子集，每个数据子集中都只有一个数据，整个数据集会被划分为k类
    &lt;ul&gt;
      &lt;li&gt;$n_A = N,\quad N_{a_i} = 1,\quad N_{c} = k $&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;对每个类别分类结果，只有是或者不是两种结果
    &lt;ul&gt;
      &lt;li&gt;$N_{a_i,k} = 0\quad or\quad 1$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;所以每个子类别的信息熵都为0
    &lt;ul&gt;
      &lt;li&gt;$\frac{N_{a_i,l}}{N_{a_i}}log\frac{N_{a_i,l}}{N_{a_i}} = 0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这使得信息增益达到最大值，但显然这个特征不具有分化能力，改进该问题，我们有了信息增益率。&lt;/p&gt;

&lt;h4 id=&quot;信息增益率&quot;&gt;信息增益率&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;计算特征A对数据集D的拆分能力&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}SplitInfo_A(D) = -\sum_{j=1}^v \frac{|D_j|}{|D|}*log_2\frac{|D_j|}{|D|}\end{gather}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;这个公式和求信息熵类似，与$Info_A(D)$相比，修改了后面乘积部分&lt;/li&gt;
      &lt;li&gt;由于$D_j$ 表示数据集在特征$A$上取每个值$a_j$的个数，而不是数据本身的真实分类$C_j$的个数，所以它间接衡量了特征$A$的取值集合大小&lt;/li&gt;
      &lt;li&gt;特征$A$的取值集合较大时，$j$的取值较多，结果更加不确定，所以$SplitInfo_A(D)$ 较大；特征$A$的取值集合较小时$SplitInfo_A(D)$ 较小&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算特征A的信息增益率&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}GainRatio(A) = \frac{Gain(A)}{SplitInfo_A(D)}\end{gather}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;基尼系数&quot;&gt;基尼系数&lt;/h4&gt;

&lt;p&gt;基尼系数用来衡量样本被选错的概率，基尼系数越小，表示特征越好，这与前两种方法是相反的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;计算数据集D的基尼系数&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}Gini(D) =\sum_{i=1}^mp_i(1-p_i) =1 - \sum_{i=1}^mp_i^2，\quad p_i = \frac{|C_{i,D}|}{|D|}\end{gather}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$p_i(1-p_i)$ 表示第$i$类别的样本被选错的概率&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;计算数据集D经过特征A分类后的基尼系数&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather}Gini_A(D) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)\end{gather}&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;$D_1,D_2$是$D$根据特征的某个值a，划分出来的数据子集&lt;/li&gt;
      &lt;li&gt;$D_1 = {(x,y)\in D \mid x_A \le a},D_2 = D-D_1$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;连续值与缺失值处理&quot;&gt;连续值与缺失值处理&lt;/h3&gt;

&lt;h4 id=&quot;缺失值&quot;&gt;缺失值&lt;/h4&gt;

&lt;p&gt;面对的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何在属性缺失值的情况下划分属性？&lt;/li&gt;
  &lt;li&gt;给定划分属性，如果样本在该属性上的值缺失，则如何划分样本？&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;连续值&quot;&gt;连续值&lt;/h4&gt;

&lt;p&gt;遇到连续属性，我们通过下面的操作将连续特征转化为离散特征&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;假设特征$A$在$D$中出现了$v$个不同的取值, ${a_1,a_2,…,a_v}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将${a_1,a_2,…,a_v}$ 升序排列&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;选取$v-1$个划分点，每个划分点依次为$\frac{a_1 + a_{2}}{2},\frac{a_2 + a_{3}}{2},…,\frac{a_{M-1} + a_{M}}{2}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>nanbei629</name></author><category term="机器学习" /><category term="树" /><summary type="html">决策树是一种有监督模型，既能分类也能用于回归，既可以是二叉树，也可以是非二叉树。决策树的叶子结点表示结果，非叶子结点表示特征。建立决策树的过程主要是通过算法递归地为每个非叶子结点选择最优特征；建立后经过剪枝操作，将因为数据noise 和outlier生成的分支移除，以此提升树的精确度和泛化能力。决策树因为可读性强，计算快而被广泛应用。</summary></entry></feed>